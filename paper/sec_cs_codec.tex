%!TEX root = paper_ecg_cs_codec.tex
\section{Proposed Codec Architecture}
\label{sec:arch}
\begin{figure}[!ht]
\centering 
\input{codec/dg_encoder}
\caption{Digital Compressive Sensing Encoder}
\label{fig-cs-encoder}
\end{figure}

\begin{figure}[!ht]
\centering 
\input{codec/dg_decoder}
\caption{Digital Compressive Sensing Decoder}
\label{fig-cs-decoder}
\end{figure}
This section describes a codec architecture
for digital signals involving digital
compressive sensing, quantization and entropy
coding steps.
\Cref{fig-cs-encoder}
and \cref{fig-cs-decoder}
depict high-level block diagrams of the encoder
and the decoder.
The encoding algorithm is presented in
\cref{alg:encoder}.
The decoding algorithm is presented in
\cref{alg:decoder}.

\begin{figure}[!t]
 \removelatexerror
 \centering
\input{codec/alg_encoder}
\end{figure}

\begin{figure}[!t]
 \removelatexerror
 \centering
\input{codec/alg_decoder}
\end{figure}

The digital signal is split into windows
of $n$ samples each. The windows of
the digital signal are further grouped into
frames of $w$ windows each. The
last frame may have less than $w$
windows.
The encoder compresses the digital signal
frame by frame into a bitstream.
It first sends
encoding parameters in the form
of a stream header (see \cref{tbl:header:stream}).
Then for each frame of the digital signal,
it sends a frame header (see \cref{tbl:header:frame})
followed by a frame payload consisting of the
quantized and entropy-coded measurements for the frame.
The decoder initializes itself by reading
the stream header from the incoming bitstream. 
Then, it reconstructs the digital signal
frame by frame.


\input{codec/tbl_stream_header}
\input{codec/tbl_frame_header}


\subsection{Sensing Matrix}
Following \cite{mamaghanian2011compressed},
we construct a sparse binary sensing matrix $\Phi$ of size
$m \times n$.
Each column of $\Phi$ consists of exactly $d$ ones and
$m-d$ zeros, where the position of ones has been randomly
selected in each column.
Sparse binary sensing matrices can be treated as adjacency 
matrix representations of bipartite graphs.
In the equation $\by = \Phi \bx$, let $\bx$
and $\by$ represent node sets $U$ and $V$
respectively where each node in the node-set
corresponds to a component of the vector.
Consider the bipartite graph $G (\Phi) = (U, V, E)$
\footnote{$U$ and $V$ are left and right nodes of $G$
and $E$ is the set of edges between the left and right nodes.}
such that
an edge $(i, j)$ belongs to $E$ if and only if
$\Phi(j, i) = 1$. Informally, such a graph is
an expander \cite{gilbert2010sparse}
if each small subset of nodes in $U$ has many neighbors
in $V$. We refer to $U$ as the left part and $V$
as the right part of the graph $G$.
A bipartite graph is called left $d$-regular
if every node in the left part has exactly
$d$ neighbors in the right part. This corresponds to having
exactly $d$ ones in each column of $\Phi$.
A bipartite left-$d$ regular graph $G = (U, V, E)$
is called a $(k, d, \epsilon)$-expander if any
set $S \subset U$ of at most $k$ left nodes has
at least $(1-\epsilon)d|S|$ neighbors.

Sparse binary sensing matrices cannot satisfy the standard RIP
property \cite{candes2008restricted},
however, they do satisfy a generalized
RIP-p property \cite{berinde2008combining,gilbert2010sparse}.
An $m \times n$ matrix $\Phi$ is said to satisfy
RIP ($p, k, \delta$) if for any $k$-sparse vector
$\bx$, we have

\begin{equation}
\| \bx \|_p (1 - \delta) \leq \| \Phi \bx \|_p \leq \| \bx \|_p.
\end{equation}
The adjacency matrices of expander graphs have
this property. Gilbert et al. show in \cite{gilbert2010sparse}
that if $\Phi$ is the adjacency matrix of a $(k,d, \epsilon)$
expander graph $G = (U, V, E)$, then the scaled matrix
$\Phi / d$ satisfies RIP $(1,k, \delta)$ property for
$\delta = 2 \epsilon$.
They further show that for an expander matrix $\Phi$,
for any signal $\bx$, given $\by = \Phi \bx$, we
can recover $\hat{\bx}$ such that

$$
\| \bx - \hat{\bx} \|_1 \leq c(\epsilon) \| \bx - \bx_{|k} \|_1
$$
where $\bx_{|k}$ is the best $k$-term approximation of $\bx$
and $c(\epsilon)$ is a constant depending on $\epsilon$.
Stable recovery guarantees are also available.
Berinde et al. show in \cite{berinde2008practical} that
for $k$-sparse $\bx$ and the measurements $\by = \Phi \bx + \be$
where the measurement noise $\be$ has an $\ell_1$ bound,
there exists an algorithm that recovers a $k$-sparse $\hat{\bx}$
such that $\| \bx - \hat{\bx} \|_1 = \bigO{\| \be \|_1 / d}$
if $\Phi$ is a matrix induced by a $(s,d, \epsilon)$-expander
$G$ where $s = \bigO{k}$. 
These theoretical results are the foundation of our encoder
design wherein a bounded quantization noise doesn't impact
the reconstruction quality. 

Mamaghanian et al. \cite{mamaghanian2011compressed}
studied the variation of reconstruction SNR with $d$ for SPGL1
algorithm \cite{van2009probing}.
They chose a value of $d=12$ below which
SPGL1 recovery suffered. Zhang et al. \cite{zhang2012compressed}
showed experimentally that the BSBL-BO algorithm can do good
recovery for much lower values of $d$.
In our experiments, we have reported results for both $d=12$
as well as $d=4$.


\subsection{Encoding}


Here we describe the encoding process for each frame.
Let a frame of digital signal be denoted by a vector $\bx$.
The frame is split into non-overlapping windows of $n$
samples each ($\{\bx_i\}_{1 \le i\le w}$).
We put them together to form the (signed) signal matrix
\footnote{
PhysioNet provides the baseline values for each channel
in their ECG records.
Since the digital samples are unsigned, we have subtracted
them by the baseline value ($1024$ for 11-bit encoding).
11 bits mean that unsigned values range from
$0$ to $2047$. The baseline for zero amplitude is
digitally represented as $1024$.
After baseline adjustment, the range of values becomes
$[-1024,1023]$.
}:
\begin{equation}
\bX = \begin{bmatrix}
\bx_1 & \bx_2 & \dots & \bx_w
\end{bmatrix}.
\end{equation}
We perform compressive sensing on the whole frame
of windows together as:
\begin{equation}
\bY = \Phi \bX.
\end{equation}
Note that by design, the sensing operation can be implemented
using just lookup and integer addition. The ones
in each row of $\Phi$ identify the samples within the window
to be picked up and summed. Consequently, $\bY$ consists of
only signed integer values.

Beyond this point, the window structure of the signal is not
relevant for quantization and entropy coding.
Hence, we flatten it into a vector $\by$ of $m w$ measurements.
\begin{equation}
\by = \text{flatten}(\bY).
\end{equation}
\subsubsection{Quantization}
The quantization for each frame is specified by a parameter $q$.
This parameter is either fixed for the whole stream
(as specified in the stream header),
or varies from frame to frame (under adaptive quantization).
It is given by:
\begin{equation}
\label{eq-enc-quantization}
\bar{\by} = \left \lfloor \frac{1}{2^q} \by \right \rfloor.
\end{equation}
For integer measurement values, quantized values are also
integers with a smaller range (by a factor of $2^q$).
It can be easily implemented on a computer as a signed
right shift by $q$ bits.
We can measure the quantization error introduced by
this step by comparing $\by$ with the inverse quantized values
$\tilde{\by} = 2^q \bar{\by}$.

If adaptive quantization has been specified, then we vary
the quantization parameter $q$ from a value $q_{\max}=6$
down to a value $q_{\min}$
till we reach a limit on $\nrmse$ \eqref{eq:n-rmse} between $\by$ and $\tilde{\by}$
as specified by the parameter $\rho$ in the stream header.

\subsubsection{Entropy Model}
We model the quantized measurements as samples from
a quantized Gaussian distribution that can only take integral values.
First, we estimate the mean $\mu_y$ and standard deviation $\sigma_y$
of measurement values in $\bar{\by}$.
We round up the values of $\mu_y$ and $\sigma_y$ to the nearest integer
for efficient encoding.
Entropy coding works with a finite alphabet.
Accordingly, the quantized Gaussian model
requires specification of the minimum
and maximum values that our quantized
measurements can take.
The range of values in $\bar{\by}$ must be clipped to this range.
The clipping function for a scalar value is defined as:
\begin{equation}
\clip (v, a, b) \triangleq \begin{cases}
a & v \leq a \\
b & v \geq b \\
v & \text{otherwise}.
\end{cases}
\end{equation}
We clip the values in $\bar{\by}$ to the range
$[\mu_y - r \sigma_y, \mu_y + r \sigma_y]$
where $r$ is the range parameter estimated for each frame.
Similar to adaptive quantization, we vary $r$ from $2$ to $8$
till we have captured sufficient variation in $\bar{\by}$
and $\nrmse(\bar{\by}, \hat{\by}) \leq \gamma$
where $\gamma$ is a parameter specified in the stream header.

The adaptive quantization and adaptive clipping ensure that
the total quantization error introduced by quantization and
clipping steps is bounded.

\subsubsection{Entropy Coding}
We use the ANS entropy coder to encode $\hat{\by}$ into an array
$\bc$ of 32-bit integers (called words).
This becomes the payload of the frame to be sent to the decoder.
The total number of bits in the frame payload
is the length of the array $n_c$ times 32.
Note that we have encoded and transmitted $\hat{\by}$
and not the unclipped $\bar{\by}$. ANS entropy coding
is a lossless encoding scheme. Hence, $\hat{\by}$
will be reproduced faithfully in the decoder if there
are no bit errors involved in the transmission
\footnote{We assume that appropriate
channel coding mechanism has been used.}.

\subsubsection{Integer Arithmetic}
The input to digital compressive sensing is a stream of integers.
The sensing process with
the sparse binary sensing matrix can be implemented
using integer sums and lookup.
It is possible to implement the computation of
approximate mean and standard deviation
using integer arithmetic.
We can use the normalized mean square error thresholds
for adaptive quantization and clipping steps under integer arithmetic.
ANS entropy coding is fully implemented using integer arithmetic.
The proposed encoder can be fully implemented using integer arithmetic.

\subsubsection{Bounded Quantization Noise}
$\nrmse$ values of $\rho$ and $\gamma$ limit
the amount of noise introduced by the quantization
and clipping steps respectively.
From \eqref{eq:snr} we can see that $\snr = -20 \log_{10}(\nrmse)$.
Our typical values are $\rho=0.01$ and $\gamma=0.02$.
They correspond to 40 dB of noise for quantization
and 34 dB of noise for the clipping step.


\subsubsection{Encoder Computational Complexity}
The encoding process is dominated by the matrix
multiplication $\by  = \Phi \bx$. For a general
sensing matrix, this operation is $\bigO{m n}$.
A binary sensing matrix has a total of $n d$
ones. Since $d$ is a small constant, hence
the total number of lookup and addition operations
is $\bigO{n}$. This significantly reduces the
computational complexity of the encoder.


\subsection{Decoding}
Decoding of a frame starts by reading the frame header
which provides the frame encoding parameters:
$\mu_y, \sigma_y, q, r, n_w, n_c$.
The frame header is used for building
the quantized Gaussian distribution model
for entropy decoding.
$n_c$ tells us the number of words ($4 n_c$ bytes) to be
read from the bitstream for the frame payload.
The ANS decoder is used to extract the encoded measurement
values $\hat{\by}$ from the frame payload.
Inverse quantization and windowing are performed
to construct the measurement matrix $\tilde{\bY}$
which is the input to a suitable sparse recovery algorithm.

The architecture is flexible in terms of the choice of the
reconstruction algorithm.
\begin{equation}
\tilde{\bX} = \mathrm{reconstruct}(\tilde{\bY}).
\end{equation}
Each column (window) in $\tilde{\bY}$ is decoded independently.
In our experiments, we have built two different algorithms:
\begin{itemize}
  \item BSBL-BO (Block Sparse Bayesian Learning-Bound Optimization)
  \cite{zhang2013extension,zhang2012compressed,zhang2016comparison}
  \item CS-NET \cite{zhang2021csnet}
\end{itemize}
Once each window has been reconstructed, they are flattened
to form the sequence of reconstructed samples.

\subsection{BSBL-BO}
Natural signals tend to have richer structures beyond
sparsity.
A common structure in natural signals
is a block/group structure \cite{eldar2010block}. 
We introduce the block/group structure on $\bx$ as
\begin{equation}
\bx = \begin{pmatrix}
\bx_1 & \bx_2 & \dots & \bx_g
\end{pmatrix}
\end{equation}
where each $\bx_i$ is a block of $b$ values.
The signal $\bx$ consists of $g$ such blocks.
Under the block sparsity model, only a few $k \ll g$
blocks are nonzero (active) in the signal $\bx$
however, the locations of these blocks are unknown.
We can rewrite the sensing equation as:
\begin{equation}
\by = \sum_{i=1}^g \Phi_i \bx_i + \be
\end{equation}
by splitting the sensing matrix into blocks of columns appropriately.
$\be$ denotes the measurement error.

Under the sparse Bayesian framework \cite{zhang2013extension},
each block is assumed to satisfy a parametrized multivariate
Gaussian distribution:
\begin{equation}
\PP(\bx_i ; \gamma_i, \bB_i) 
= \NNN(\bzero, \gamma_i \bB_i), \Forall i=1,\dots,g
\end{equation}
with the unknown parameters $\gamma_i$ and $\bB_i$.
$\gamma_i$ is a non-negative parameter controlling the
block sparsity of $\bx$. When the block sparse Bayesian
model for $\bx$ is estimated, most $\gamma_i$ tend to
be zero due to automatic relevance determination
\cite{tipping2001sparse} promoting block sparsity.
$\bB_i \in \RR^{b \times b}$ is a positive definite
matrix, capturing the correlation structure of the $i$-th
block.
We further assume that the blocks are mutually uncorrelated.
The prior of $\bx$ can then be written as
\begin{equation}
\PP(\bx; \{ \gamma_i, \bB_i\}_i ) = \NNN(\bzero, \Sigma_0)
\end{equation}
where
\begin{equation}
\Sigma_0 = \text{diag}\{\gamma_1 \bB_1, \dots, \gamma_g \bB_g \}.
\end{equation}
We also model the correlation among the values
within each active block as an AR-1 process
with a common model parameter.
Under this assumption the matrices
$\bB_i$ take the form of a Toeplitz matrix
\begin{equation}
\bB_i = \bB = \begin{bmatrix}
1 & r & \dots & r^{b-1}\\
r & 1 & \dots & r^{b-2}\\
\vdots &  & \ddots & \vdots\\
r^{b-1} & r^{b-2} & \dots & 1
\end{bmatrix}
\end{equation}
where $r$ is the AR-1 model coefficient.
This constraint significantly reduces
the number of model parameters to be learned.

Measurement error is modeled as an independent zero-mean Gaussian
noise $\PP(\be; \lambda) \sim \NNN(\bzero, \lambda \bI)$.
BSBL doesn't require us to provide the value of noise variance
as input.
It is able to estimate $\lambda$ within the algorithm.
The estimate of $\bx$ under the Bayesian learning framework
is given by the posterior mean of $\bx$ given the measurements $\by$.

Our implementation of the BSBL-BO algorithm is available as part of
CR-Sparse library \cite{kumar2021cr}.
As Zhang et al. suggest in \cite{zhang2012compressed},
block sizes are user-defined and they are identical and
no pruning of blocks is applied. Our implementation has been
done under these assumptions and is built using JAX so that it can
be run on GPU hardware easily to speed up decoding.
The only configurable parameter for this decoder is the block size
which we shall denote by $b$ in the following.

\subsection{CSNet}

CSNet \cite{zhang2021csnet} is a state-of-the-art deep learning network
for the reconstruction of ECG signals from compressive
measurements. The measurements are first raised back
to $\RR^n$ as $\bz = \Phi^T \bx$. 
An initial reconstruction module consists of three convolution
layers. It is followed by the secondary reconstruction module
consisting of an LSTM layer followed by a dense layer.
We implemented this network and followed the training procedure
as described in \cite{zhang2021csnet}.
Our primary change was that we didn't feed the original
measurements.
Rather, we trained the network with $\tilde{\bY}$
as inputs and $\bX$ as expected outputs.
Thus, we tested whether CSNet can work well with our
quantized and clipped measurements. 
Another difference from \cite{zhang2021csnet} is that
we use sparse binary sensing matrices rather than standard RIP compliant
sensing matrices.

