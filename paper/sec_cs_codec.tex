%!TEX root = paper_ecg_cs_codec.tex
\section{Proposed Codec Architecture}
\label{sec:arch}
\begin{figure}[!ht]
\centering 
\input{codec/dg_encoder}
\caption{Digital Compressive Sensing Encoder}
\label{fig-cs-encoder}
\end{figure}

\begin{figure}[!ht]
\centering 
\input{codec/dg_decoder}
\caption{Digital Compressive Sensing Decoder}
\label{fig-cs-decoder}
\end{figure}
This section describes a codec architecture
for digital signals involving digital
compressive sensing, quantization and entropy
coding steps.

\Cref{fig-cs-encoder}
and \cref{fig-cs-decoder}
depict high-level block diagrams of the encoder
and the decoder.
The encoding algorithm is presented in
\cref{alg:encoder}.
The decoding algorithm is presented in
\cref{alg:decoder}.

\begin{figure}[!t]
 \removelatexerror
 \centering
\input{codec/alg_encoder}
\end{figure}

\begin{figure}[!t]
 \removelatexerror
 \centering
\input{codec/alg_decoder}
\end{figure}

The digital signal is split into windows
of $n$ samples each. The windows of
the digital signal are further grouped into
frames of $w$ windows each. The
last frame may have less than $w$
windows.

The encoder compresses the digital signal
frame by frame into a bitstream.
It first sends
encoding parameters in the form
of a stream header (see \cref{tbl:header:stream}).
Then for each frame of the digital signal,
it sends a frame header (see \cref{tbl:header:frame})
followed by a frame payload consisting of the
quantized and entropy-coded measurements for the frame.

The decoder initializes itself by reading
the stream header from the incoming bitstream. 
Then, it reconstructs the digital signal
frame by frame.


\input{codec/tbl_stream_header}
\input{codec/tbl_frame_header}


\subsection{Sensing Matrix}
Following \cite{mamaghanian2011compressed},
we construct a sparse binary sensing matrix $\Phi$ of size
$m \times n$.
Each column of $\Phi$ consists of exactly $d$ ones and
$m-d$ zeros, where the position of ones has been randomly
selected in each column. Identical algorithms are used
to generate the sensing matrix using the stream header
parameters in both the encoder and the decoder.
The randomization is seeded with the parameter $\mathrm{key}$
sent in the stream header.

\subsection{Encoding}


Here we describe the encoding process for each frame.
Let a frame of digital signal be denoted by a vector $\bx$.
The frame is split into non-overlapping windows of $n$
samples each ($\{\bx_i\}_{1 \le i\le w}$).
We put them together to form the (signed) signal matrix
\footnote{
PhysioNet provides the baseline values for each channel
in their ECG records.
Since the digital samples are unsigned, we have subtracted
them by the baseline value ($1024$ for 11-bit encoding).
11 bits mean that unsigned values range from
$0$ to $2047$. The baseline for zero amplitude is
digitally represented as $1024$.
After baseline adjustment, the range of values becomes
$[-1024,1023]$.
}:
\begin{equation}
\bX = \begin{bmatrix}
\bx_1 & \bx_2 & \dots & \bx_w
\end{bmatrix}.
\end{equation}
We perform compressive sensing on the whole frame
of windows together as:
\begin{equation}
\bY = \Phi \bX.
\end{equation}
Note that by design, the sensing operation can be implemented
using just lookup and integer addition. The ones
in each row of $\Phi$ identify the samples within the window
to be picked up and summed. Consequently, $\bY$ consists of
only signed integer values.

Beyond this point, the window structure of the signal is not
relevant for quantization and entropy coding.
Hence, we flatten it into a vector $\by$ of $m w$ measurements.
\begin{equation}
\by = \text{flatten}(\bY).
\end{equation}
\subsubsection{Quantization}
The quantization for each frame is specified by a parameter $q$.
This parameter is either fixed for the whole stream
(as specified in the stream header),
or varies from frame to frame (under adaptive quantization).
It is given by:
\begin{equation}
\bar{\by} = \left \lfloor \frac{1}{2^q} \by \right \rfloor.
\end{equation}
For integer measurement values, quantized values are also
integers with a smaller range (by a factor of $2^q$).
It can be easily implemented on a computer as a signed
right shift by $q$ bits.
We can measure the quantization error introduced by
this step by comparing $\by$ with the inverse quantized values
$\tilde{\by} = 2^q \bar{\by}$.

If adaptive quantization has been specified, then we vary
the quantization parameter $q$ from a value $q_{\max}=6$
down to a value $q_{\min}$
till we reach a limit on $\nrmse$ between $\by$ and $\tilde{\by}$
as specified by the parameter $\rho$ in the stream header.

\subsubsection{Entropy Model}
We model the quantized measurements as samples from
a quantized Gaussian distribution that can only take integral values.
First, we estimate the mean $\mu_y$ and standard deviation $\sigma_y$
of measurement values in $\bar{\by}$.
We round up the values of $\mu_y$ and $\sigma_y$ to the nearest integer
for efficient encoding.
Entropy coding works with a finite alphabet.
Accordingly, the quantized Gaussian model
requires specification of the minimum
and maximum values that our quantized
measurements can take.
The range of values in $\bar{\by}$ must be clipped to this range.
The clipping function for a scalar value is defined as:
\begin{equation}
\clip (v, a, b) \triangleq \begin{cases}
a & v \leq a \\
b & v \geq b \\
v & \text{otherwise}.
\end{cases}
\end{equation}
We clip the values in $\bar{\by}$ to the range
$[\mu_y - r \sigma_y, \mu_y + r \sigma_y]$
where $r$ is the range parameter estimated for each frame.
Similar to adaptive quantization, we vary $r$ from $2$ to $8$
till we have captured sufficient variation in $\bar{\by}$
and $\nrmse(\bar{\by}, \hat{\by}) \leq \gamma$
where $\gamma$ is a parameter specified in the stream header.

The adaptive quantization and adaptive clipping ensure that
the total quantization error introduced by quantization and
clipping steps is bounded.

\subsubsection{Entropy Coding}
We use the ANS entropy coder to encode $\hat{\by}$ into an array
$\bc$ of 32-bit integers (called words).
This becomes the payload of the frame to be sent to the decoder.
The total number of bits in the frame payload
is the length of the array $n_c$ times 32.
Note that we have encoded and transmitted $\hat{\by}$
and not the unclipped $\bar{\by}$. ANS entropy coding
is a lossless encoding scheme. Hence, $\hat{\by}$
will be reproduced faithfully in the decoder if there
are no bit errors involved in the transmission
\footnote{We assume that appropriate
channel coding mechanism has been used.}.

\subsubsection{Integer Arithmetic}
The input to digital compressive sensing is a stream of integers.
The sensing process with
the sparse binary sensing matrix can be implemented
using integer sums and lookup.
It is possible to implement the computation of
approximate mean and standard deviation
using integer arithmetic.
We can use the normalized mean square error thresholds
for adaptive quantization and clipping steps under integer arithmetic.
ANS entropy coding is fully implemented using integer arithmetic.
The proposed encoder can be fully implemented using integer arithmetic.

\subsection{Decoding}
Decoding of a frame starts by reading the frame header
which provides the frame encoding parameters:
$\mu_y, \sigma_y, q, r, n_w, n_c$.
The frame header is used for building
the quantized Gaussian distribution model
for entropy decoding.
$n_c$ tells us the number of words ($4 n_c$ bytes) to be
read from the bitstream for the frame payload.
The ANS decoder is used to extract the encoded measurement
values $\hat{\by}$ from the frame payload.
Inverse quantization and windowing are performed
to construct the measurement matrix $\tilde{\bY}$
which is the input to a suitable sparse recovery algorithm.

The architecture is flexible in terms of the choice of the
reconstruction algorithm.
\begin{equation}
\tilde{\bX} = \mathrm{reconstruct}(\tilde{\bY}).
\end{equation}
Each column (window) in $\tilde{\bY}$ is decoded independently.
In our experiments, we have built two different algorithms:
\begin{itemize}
  \item BSBL-BO (Block Sparse Bayesian Learning-Bound Optimization)
  \cite{zhang2013extension,zhang2012compressed,zhang2016comparison}
  \item CS-NET \cite{zhang2021csnet}
\end{itemize}
Once each window has been reconstructed, they are flattened
to form the sequence of reconstructed samples.

\subsection{BSBL-BO}
Our implementation of the BSBL-BO algorithm is available as part of
CR-Sparse library \cite{kumar2021cr}.
As Zhang et al. suggest in \cite{zhang2012compressed},
block sizes are user-defined and they are identical and
no pruning of blocks is applied. Our implementation has been
done under these assumptions and is built using JAX so that it can
be run on GPU hardware easily to speed up decoding.
The only configurable parameter for this decoder is the block size
which we shall denote by $b$ in the following.
Once the samples have been decoded,

\subsection{CS-NET}
We trained the network with $\tilde{\bY}$
as inputs and $\bX$ as expected outputs.

